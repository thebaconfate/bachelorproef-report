\documentclass[a4paper]{article}

% Load the VUB package.
% This has many options, please read the documentation at
% https://gitlab.com/rubdos/texlive-vub
\usepackage{vub}
\usepackage[english]{babel}

% Some highly suggested packages, please read their manuals.
\usepackage{hyperref}
\usepackage{float}
\usepackage{cleveref}
\usepackage[natbib, style=apa]{biblatex} %perhaps use ieee style? 
\usepackage{csquotes}
\addbibresource{bibliography.bib}
\setlength\parskip{\baselineskip}

\title{Delivering systems with Stork}
\pretitle{\flushleft{Bachelor thesis submitted in partial fulfillment of the requirements for the degree of bachelor of science: Computer Science}}
\subtitle{ A distributed computing deployment tool}
\author{GÃ©rard Lichtert}
\date{\today}
\promotors{Promotors: Prof.\ Dr.\ Joeri de Koster and Prof.\ Dr.\ Wolfgang de Meuter. \and Advisor: Mathijs Saey}
\faculty{Sciences and bioengineering sciences}

\begin{document}
\maketitle
\tableofcontents
\newpage
\raggedright{}


\section{Introduction}
In a world of electronics and machines where power consumption is always increasing, optimizing power consumption is becoming increasingly important. While electricity is expensive, the main reason lies in climate change. According to \cite{owid-energy-mix}, the majority of our electricity production still comes from non-renewable sources, such as coal, oil, and gas. These sources produce a lot of CO\textsuperscript{2}, and other greenhouse gases, which are the main contributors to climate change. While there is research being done to optimize energy generation, optimizing power consumption is becoming increasingly important. This means that we as programmers can also play a role in optimizing our programs to consume less energy. In the world of computing, according to \cite{cloudcomputingenergycrisis} cloud computing takes 1\% of worldwide energy consumption. While this may not seem like much, it is still a significant amount of energy.

Computing like most electronics, require electricity to function, however when computers communicate with each other they also have to send data, or make requests through the Internet. This leads to a very high network usage, since in today's world, we are unequivocally connected with each other. Furthermore, the amount of devices using the internet keeps growing, which leads to even more network usage. Higher network usage also leads to higher energy consumption, which leads to a higher carbon footprint, as stated in \cite{RATHEESH}. \cite{datavolumeeffects} states that there is also an increase in energy consumption due to the infrastructure required for the increase in data volume. To reduce the network load we need to look at the data that is being sent through the network and if we can reduce it.

Data is usually transported through the network for a few reasons. Sometimes it is to send data to a device to update local data, like a chat message that needs to be added to the chat, or a new email that needs to be downloaded. While often compressed, the data is used as-is, and thus cannot be further reduced. Other times, however, data is sent to be processed. This can potentially be optimized by applying the edge-computing principle. This means that (part of) the data that is originally meant for processing is processed locally first, potentially reducing the amount of data that needs to be sent after preprocessing it. Logically, if the data is smaller after preprocessing, the network load should be reduced, and consequently the energy consumption. However, for a certain set of devices it could be optimal to pre-process the complete set of data prior to sending it over the network. While for another set of devices it could be optimal to send the data directly to the server. Sometimes the optimal configuration could be a combination of the two. This gives rise to the question of how we can easily declare where which data gets processed.

In this thesis we introduce a tool for this purpose: Stork. Stork is a distributed computing deployment tool that makes it possible to deploy distributed systems in a declarative way. It allows us to change the deployment configuration as well as declare where which data gets processed. It does so by delivering the correct parts of the program to the correct devices. Hence, the name Stork.

We will know that the tool is successful if we can use the tool as a library to deploy a distributed system without manual intervention. Furthermore, the tool should allow us to change the configuration of the deployment, and declare where which data gets processed. Lastly the data must correctly be processed on the declared devices.
\section{Background}
To start building Stork we need to look at the goals that our tool should achieve. As mentioned in the introduction, we require a tool that allows us to declare where which data gets processed. This means that we need to be able to deploy parts of our program to different devices. Consequently, this means that our tool needs to work in the context of distributed systems. For this we need to choose a distributed computing paradigm that allows us to deploy our system in a distributed way, as well as change where parts of our programs reside.
Next, we need to choose a programming language of implementation, while keeping several factors in mind, such as, popularity, the use case of the programming language and the available libraries or technologies that exist in the language for our chosen paradigm.
After choosing our programming language, we need to look at the available technologies and libraries in the programming language and choose the one(s) that best fit our needs. It could happen that we need other libraries or technologies to support the ones we choose but, we will cross that bridge when we get there.
\subsection{Distributed computing paradigm}
To start with distributed computing, we have to look for a suitable distributed computing paradigm that allows our system to be deployed from the cloud without much manual intervention, yet is able to do what we require it to do. There are several options, such as:
\begin{enumerate}
    \item Message Passing Interface (MPI) as described in \cite{MPI}
    \item Remote Procedure Call (RPC) as described in \cite{RPC}
    \item Shared Memory Model as described in \cite{SMM}
    \item The Actor Model as described in \cite{ActorModel}
    \item Publish/Subscribe (Pub/Sub) as described in \cite{PubSub}
\end{enumerate}
While each paradigm has their strengths and drawbacks, we will be using the Actor Model. Primarily because its modularity. Each actor encapsulates its state and behavior, which makes it a prime candidate to encapsulate the behavior of the parts that process certain parts of data. This allows us to easily move part of the data processing pipeline from one device to another. Which helps with our goal of declaring where which data gets processed. Interestingly, it can also encapsulate the behavior of IoT devices, which according to \cite{differentnetworkneedsiot} will account for 50\% of all networked devices by 2023. Seeing as IoT devices are becoming increasingly important, they should be a factor in our choice of distributed computing paradigm.

While it is technically possible to achieve modularity in MPI, RPC, and the Shared Memory Model, due to the tight coupling of those models, it is harder to achieve. This is because the state and behavior of the system are not encapsulated in a single entity, but rather spread across the entire system. This makes it harder to move parts of the system around. The Pub/Sub model is also a good candidate, however, it is more suited for event-driven systems, rather than systems with direct communication.

Other than the modularity, we also choose it for its maintainability. Since each part of the system is encapsulated in an Actor, we can easily change the behavior of parts of the system, instead of having to change our entire system. Another reason is scalability. We are able to distribute actors across different systems and machines. Which makes it perfect for moving parts of the data processing. Fault-tolerance makes it easier to contain errors and within individual actors and not propagate this to the entire system. The asynchronous nature of the Actor Model allows us to sparely use resources as needed, which will presumably reduce the required energy of the system, helping with the energy optimization. Lastly, through the message-based communication between Actors, we can easily decouple components, which in turn makes it easier to debug and test our systems. This is beneficial because distributed systems can quickly become very complex.
\subsection{Programming language of implementation}
The next step is to choose a programming language that supports the Actor Model. However, before listing the programming languages that support the Actor model, we need to consider a few things first. We want our tool to be maintainable. This means that choosing a programming language that is popular and has a large community is important. Furthermore, when processing data, companies tend to analyze the data, and perform data science on it. This means that ideally we should choose a programming language that is also used in data science, so that users of Stork can stay in the same language. Lastly but more importantly, we should choose a programming language that has libraries or technologies that support the Actor Model. This is important because otherwise we have to make our own implementation of the Actor Model, which is not ideal, considering there are already several implementations available.

A quick google search yields the following programming languages that support the Actor Model:
\begin{enumerate}
    \item Scala with its Akka library
    \item Java with its Akka library
    \item Erlang, where the Actor Model is built into the language
    \item Elixir, where the Actor model is built into the language, and based on Erlang, Open telecom platform (OTP)
    \item JavaScript with its js.Actor library
    \item Python with its Pykka and Thespianpy libraries
    \item C\# with its Akka.NET library
    \item Rust with its Actix library
\end{enumerate}

In terms of popularity, according to \cite{stackoverflowsurvey}, they are ranked as follows\footnote{TypeScript is emitted since it uses JavaScript libraries}:
\begin{enumerate}
    \item JavaScript
    \item Python
    \item Java
    \item C\#
    \item Rust
    \item Scala
    \item Elixir
    \item Erlang
\end{enumerate}
With the last three being used by less than \(5\%\) of the respondents, and the top two being used by roughly \(49\%-63\%\) of respondents, we can focus on the top two. To stay in the same programming language that is often used by data scientists, we choose Python. This because according to \cite{datasciencelanguage}, Python is the most popular language for data science, and while JavaScript is also used for data science, it does not offer nearly as much data science libraries as Python does.
\subsection{Technologies and libraries}
As mentioned when choosing the programming language of implementation, Python offers two Actor Model libraries. Pykka and Thespianpy. Setting Pykka and thespian next to each other and comparing them yields the following results:
\subsubsection{Pykka}
Quoting the documentation of the library: \enquote{Pykka is a Python implementation of the actor model. The actor model introduces some simple rules to control the sharing of state and cooperation between execution units, which makes it easier to build concurrent applications.} However, reading further down in the documentation we see that Pykka has some shortcomings, as it does not support linking actors, supervisors, or supervisor groups. It does not support communicating with actors running on other hosts, which is a requirement for our tool. Lastly, it does not come with a set of predefined message router, which would mean that we have to implement this ourselves. Notably it does have \(~1200\) stars on Github, which comparing it to Thespianpy, is more.
\subsubsection{Thespianpy}
The documentation states that: \enquote{Thespian is a Python library providing a framework for developing concurrent, distributed, fault tolerant applications. It is built on the Actor Model which allows applications to be written as a group of independently executing but cooperating "Actors" which communicate via messages. These Actors run within the Actor System provided by the Thespian library.} Since all Actors run independently within the Actor System, we can create concurrent applications. Thespianpy also allows us to run Actors independently anywhere. This means that we can have multiple servers running Thespianpy and an Actor can be run in any of these systems. Thespianpy will handle the communication between the Actors and the management process of distributing the Actors across the system. Which is something we would have to implement ourselves if we were to user Pykka.
\printbibliography
\end{document}
